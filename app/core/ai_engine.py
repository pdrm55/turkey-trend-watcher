import os
import sys
import logging
import faulthandler
import uuid
import shutil
import requests
import json
from datetime import datetime, timedelta

# Û±. ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø¯ÛŒØ§Ø¨ Ø®Ø·Ø§ÛŒ Ø³ÛŒØ³ØªÙ…ÛŒ (Ø¨Ø±Ø§ÛŒ Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ SegFaultÙ‡Ø§ Ø¯Ø± Ù…Ø­ÛŒØ· Ø¯Ø§Ú©Ø±)
faulthandler.enable()

# Û². ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø­ÛŒØ§ØªÛŒ Ø¨Ø±Ø§ÛŒ ØªÚ©â€ŒÙ†Ø® Ú©Ø±Ø¯Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø±ÛŒØ§Ø¶ÛŒ (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¯Ø§Ø®Ù„ Ø­Ø§ÙØ¸Ù‡)
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Û³. Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø§Ú˜ÙˆÙ„ Posthog Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù„ÙˆØ¯ Ø´Ø¯Ù† ØªÙ„Ù‡â€ŒÙ…ØªØ±ÛŒ Ù†Ø§Ø®ÙˆØ§Ø³ØªÙ‡
from unittest.mock import MagicMock
sys.modules["posthog"] = MagicMock()

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯Ø±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Û´. Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† (Ù¾Ø³ Ø§Ø² ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø­ÛŒØ·ÛŒ)
import torch
torch.set_num_threads(1) 

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØªØµØ§Ù„ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ---
CHROMA_HOST = os.getenv("CHROMA_HOST", "ttw_chroma")
CHROMA_PORT = os.getenv("CHROMA_PORT", "8000")
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://ttw_ollama:11434/api/generate")
LOCAL_MODEL_NAME = "qwen2.5:1.5b"

class AIEngine:
    def __init__(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ùˆ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ"""
        print("ğŸ§  Loading Multilingual Embedding Model...", flush=True)
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¹Ø§Ù„ÛŒ Ø§Ø² Ø²Ø¨Ø§Ù† ØªØ±Ú©ÛŒ
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device='cpu')
        
        try:
            self.chroma_client = chromadb.HttpClient(
                host=CHROMA_HOST,
                port=int(CHROMA_PORT),
                settings=Settings(anonymized_telemetry=False, allow_reset=True)
            )
            # Ø§ÛŒØ¬Ø§Ø¯ ÛŒØ§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ú©Ø§Ù„Ú©Ø´Ù† Ø¨Ø§ ÙØ¶Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ (Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†)
            self.collection = self.chroma_client.get_or_create_collection(
                name="news_clusters",
                metadata={"hnsw:space": "cosine"}
            )
            print(f"âœ… AI Engine Phase 3 Ready (Rolling Cache Enabled)", flush=True)
        except Exception as e:
            print(f"âŒ ChromaDB Connection Error: {e}")

    def get_embedding(self, text: str):
        """ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ (Embedding)"""
        try:
            if not isinstance(text, str): text = str(text)
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ
            vector = self.model.encode(text, convert_to_numpy=True).tolist()
            return vector
        except Exception as e:
            logger.error(f"Embedding Error: {e}")
            raise e

    def ask_local_llm(self, reference_news, candidate_news):
        """ØªØ§ÛŒÛŒØ¯ Ù†Ù‡Ø§ÛŒÛŒ Ø´Ø¨Ø§Ù‡Øª Ø¯Ùˆ Ø®Ø¨Ø± ØªÙˆØ³Ø· Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Qwen Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡"""
        prompt = f"""
        Act as a strict news editor. Compare these two Turkish news texts.
        Do they report the EXACT SAME specific incident/event occurring at the same time?
        
        If it's a new update about an old event, answer: false.
        If it's the exact same report, answer: true.
        
        Ref News: "{reference_news[:700]}"
        New News: "{candidate_news[:700]}"
        
        Answer ONLY JSON: {{"match": true}} or {{"match": false}}
        """
        payload = {
            "model": LOCAL_MODEL_NAME, "prompt": prompt, "stream": False, "format": "json",
            "options": {"temperature": 0.0, "num_ctx": 2048}
        }
        try:
            response = requests.post(OLLAMA_API_URL, json=payload, timeout=10)
            result = response.json()
            return json.loads(result['response']).get("match", False)
        except Exception as e:
            logger.error(f"Local LLM Verification Failed: {e}")
            return False 

    def get_cluster_reference_doc(self, cluster_id):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ØªÙ† Ù…Ø±Ø¬Ø¹ (Ø§ØµÙ„ÛŒâ€ŒØªØ±ÛŒÙ† Ø®Ø¨Ø±) ÛŒÚ© Ú©Ù„Ø§Ø³ØªØ± Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ"""
        try:
            # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø³Ù†Ø¯ÛŒ Ú©Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø±Ø¬Ø¹ ØªÚ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª
            result = self.collection.get(
                where={"$and": [{"cluster_id": cluster_id}, {"is_reference": True}]},
                limit=1
            )
            if result['documents'] and len(result['documents']) > 0:
                return result['documents'][0]
            
            # Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ù…Ø±Ø¬Ø¹ ØµØ±ÛŒØ­ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø§ÙˆÙ„ÛŒÙ† Ø³Ù†Ø¯ Ú©Ù„Ø§Ø³ØªØ± Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
            fallback = self.collection.get(where={"cluster_id": cluster_id}, limit=1)
            if fallback['documents'] and len(fallback['documents']) > 0:
                return fallback['documents'][0]
        except Exception as e:
            logger.error(f"Reference Doc Fetch Error: {e}")
        return None

    def process_news(self, raw_text: str, source: str, external_id: str):
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± ÙˆØ±ÙˆØ¯ÛŒ: ÙˆÚ©ØªÙˆØ±ÛŒØ²Ù‡ Ú©Ø±Ø¯Ù†ØŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ø§Ø³ØªØ± Ù…Ø´Ø§Ø¨Ù‡ Ùˆ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ÛŒØ§ Ø§Ù„Ø­Ø§Ù‚ Ø¨Ù‡ ØªØ±Ù†Ø¯.
        """
        from app.core.text_utils import clean_text
        cleaned_text = clean_text(raw_text)
        
        # Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† Ù…ØªÙˆÙ† Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆØªØ§Ù‡ ÛŒØ§ Ù†Ø§Ù…ÙÙ‡ÙˆÙ…
        if not cleaned_text or len(cleaned_text) < 25: 
            return None, False

        vector = self.get_embedding(cleaned_text)
        
        # --- ÙØ§Ø² Û³: Ø­Ø§ÙØ¸Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ù…ÛŒØ§Ù†â€ŒÙ…Ø¯Øª (Rolling Cache Filter) ---
        # ÙÙ‚Ø· Ø§Ø®Ø¨Ø§Ø±ÛŒ Ú©Ù‡ Ø¯Ø± Û´Û¸ Ø³Ø§Ø¹Øª Ú¯Ø°Ø´ØªÙ‡ Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³ØªØ±Ø³Ø§Ø²ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
        time_threshold = (datetime.now() - timedelta(hours=48)).isoformat()
        
        try:
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ChromaDB Ø¨Ø§ ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ùˆ Ø³Ø±Ø¹Øª
            results = self.collection.query(
                query_embeddings=[vector],
                n_results=5,
                where={"timestamp": {"$gte": time_threshold}}, # ÙÛŒÙ„ØªØ± Ø­Ø§ÙØ¸Ù‡ ØºÙ„ØªØ§Ù†
                include=["metadatas", "distances", "documents"]
            )
        except Exception as e:
            logger.error(f"Vector Search Query Error: {e}")
            return None, False

        cluster_id = None
        is_duplicate = False
        checked_clusters = set()

        if results['distances'] and results['distances'][0]:
            for i, distance in enumerate(results['distances'][0]):
                # Ø§Ú¯Ø± ÙØ§ØµÙ„Ù‡ Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨ÛŒØ´ Ø§Ø² 0.42 Ø¨Ø§Ø´Ø¯ØŒ ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¶Ø¹ÛŒÙ Ø§Ø³Øª
                if distance > 0.42: continue
                
                metadata = results['metadatas'][0][i]
                candidate_cluster_id = metadata['cluster_id']
                
                if candidate_cluster_id in checked_clusters: continue
                checked_clusters.add(candidate_cluster_id)

                # Ø¯Ø±ÛŒØ§ÙØª Ù…ØªÙ† Ù…Ø±Ø¬Ø¹ Ú©Ù„Ø§Ø³ØªØ± Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
                target_text = self.get_cluster_reference_doc(candidate_cluster_id) or results['documents'][0][i]
                
                # Ø­Ø§Ù„Øª Ø§ÙˆÙ„: Ø´Ø¨Ø§Ù‡Øª Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§ (Ú©Ù¾ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…)
                if distance < 0.07:
                    cluster_id = candidate_cluster_id
                    is_duplicate = True
                    break

                # Ø­Ø§Ù„Øª Ø¯ÙˆÙ…: Ø´Ø¨Ø§Ù‡Øª Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø®Ø§Ú©Ø³ØªØ±ÛŒ -> ØªØ§ÛŒÛŒØ¯ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ø­Ù„ÛŒ
                if self.ask_local_llm(target_text, cleaned_text):
                    cluster_id = candidate_cluster_id
                    is_duplicate = True
                    break

        # Ûµ. ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ØªØ±Ù†Ø¯ Ø¬Ø¯ÛŒØ¯ ÛŒØ§ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† Ø¨Ù‡ Ù‚Ø¨Ù„ÛŒ
        is_new_reference = False
        if not cluster_id:
            cluster_id = str(uuid.uuid4())
            is_new_reference = True 
            logger.info(f"âœ¨ New Trend Created: {cluster_id[:8]}")
        else:
            logger.info(f"ğŸ”— Appended to Trend: {cluster_id[:8]}")

        # Û¶. Ø°Ø®ÛŒØ±Ù‡ Ø®Ø¨Ø± Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ
        self.collection.add(
            documents=[cleaned_text],
            embeddings=[vector],
            metadatas=[{
                "source": source,
                "cluster_id": cluster_id,
                "external_id": external_id,
                "timestamp": datetime.now().isoformat(),
                "is_reference": is_new_reference
            }],
            ids=[str(uuid.uuid4())]
        )
        
        return cluster_id, is_duplicate

    def get_related_trends(self, cluster_id, limit=4):
        """ÛŒØ§ÙØªÙ† ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· (Related News) Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¯Ø± Ú©Ù„ ØªØ§Ø±ÛŒØ®Ú†Ù‡"""
        try:
            ref_doc = self.get_cluster_reference_doc(cluster_id)
            if not ref_doc: return []

            query_vector = self.get_embedding(ref_doc)

            # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¢Ø±Ø´ÛŒÙˆ Ù‡Ù… Ø¨Ø±Ø±Ø³ÛŒ Ø´ÙˆØ¯
            results = self.collection.query(
                query_embeddings=[query_vector],
                n_results=limit + 10,
                include=["metadatas"]
            )

            related_clusters = []
            seen_ids = {cluster_id}

            if results['metadatas'] and results['metadatas'][0]:
                for metadata in results['metadatas'][0]:
                    cid = metadata['cluster_id']
                    if cid not in seen_ids:
                        related_clusters.append(cid)
                        seen_ids.add(cid)
                    if len(related_clusters) >= limit:
                        break
            
            return related_clusters
        except Exception as e:
            logger.error(f"Related Trends Error: {e}")
            return []

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ ÛŒÚ©ØªØ§ (Singleton) Ø§Ø² Ù…ÙˆØªÙˆØ±
ai_engine = AIEngine()