import sys
import os
import time
import json
import re
import csv
from datetime import datetime, timezone
from google import genai
from google.genai import types

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
from app.database.models import SessionLocal, Trend, RawNews
from sqlalchemy import desc
from app.config import Config
from app.core.indexing_utils import notify_google # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø§Ø¨Ø²Ø§Ø± Ø§ÛŒÙ†Ø¯Ú©Ø³ Ú¯ÙˆÚ¯Ù„
from app.core.text_utils import slugify_turkish # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø³Ø¦Ùˆ

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú¯ÙˆÚ¯Ù„ ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    print("âŒ Error: GOOGLE_API_KEY not found in .env")

client = None
MODEL_NAME = None 
LOG_FILE = "ai_monitor_data.csv"
BASE_SITE_URL = "https://trendiatr.com" # Ø¢Ø¯Ø±Ø³ Ø§ØµÙ„ÛŒ Ø³Ø§ÛŒØª Ø¨Ø±Ø§ÛŒ Ø³Ø¦Ùˆ

# Ø­Ø¯ Ù†ØµØ§Ø¨ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ú¯ÙˆÚ¯Ù„ (Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø³Ù‡Ù…ÛŒÙ‡ Quota)
GOOGLE_INDEXING_THRESHOLD = 30

# Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯Ø³Ø§Ø²ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø®Ø¨Ø§Ø± Ø²Ø±Ø¯ (ÙØ§Ù„ Ùˆ Ø·Ø§Ù„Ø¹â€ŒØ¨ÛŒÙ†ÛŒ)
JUNK_KEYWORDS = ['burÃ§', 'fal ', 'gÃ¼nlÃ¼k burÃ§', 'astroloji', 'horoskop']

# --- Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ù„Ø§Ú¯ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ ---
if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["timestamp", "trend_id", "model", "input_tokens", "output_tokens", "duration_sec", "category", "status", "cost_usd"])

def log_to_csv(trend_id, model, in_tok, out_tok, duration, category, status):
    """Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù…Ø§Ø± Ø¯Ø± ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯"""
    try:
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡ ØªÙ‚Ø±ÛŒØ¨ÛŒ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø±Ø® Flash Lite)
        # Input: $0.075 / 1M | Output: $0.30 / 1M
        cost = (in_tok * 0.000000075) + (out_tok * 0.00000030)
        
        with open(LOG_FILE, mode='a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                trend_id,
                model,
                in_tok,
                out_tok,
                f"{duration:.2f}",
                category,
                status,
                f"{cost:.8f}"
            ])
    except Exception as e:
        print(f"âš ï¸ Log Error: {e}")

def get_best_available_model(client):
    print("ğŸ” Auto-detecting best Gemini model...")
    try:
        candidates = []
        for m in client.models.list():
            name = m.name.replace('models/', '') 
            if 'flash' in name.lower() and 'image' not in name.lower() and 'audio' not in name.lower():
                candidates.append(name)
        
        print(f"   ğŸ“‹ Candidates found: {candidates}")

        for c in candidates:
            if '1.5-flash' in c and 'latest' not in c: return c
        for c in candidates:
            if 'lite' in c and 'flash' in c:
                print(f"   ğŸ’¡ Switching to LITE model for cost savings: {c}")
                return c
        for c in candidates:
            if 'gemini-flash-latest' in c: return c
        
        if candidates: return candidates[0]
        return 'gemini-flash-latest'
        
    except Exception as e:
        print(f"âš ï¸ Could not list models: {e}. Using default.")
        return 'gemini-1.5-flash'

if GOOGLE_API_KEY:
    try:
        client = genai.Client(api_key=GOOGLE_API_KEY)
        MODEL_NAME = get_best_available_model(client)
        print(f"âœ… Selected AI Model: {MODEL_NAME}")
    except Exception as e:
        print(f"âŒ Error initializing Gemini Client: {e}")

CATEGORIES = ["Siyaset", "Ekonomi", "GÃ¼ndem", "Spor", "Teknoloji", "Sanat"]

# ==========================================
# KEYWORD LISTS (100% Turkish Alphabet - Cleaned from Persian/Arabic)
# ==========================================
SPORTS_KEYWORDS = {
    "high": ["futbol", "sÃ¼per lig", "ÅŸampiyonlar ligi", "avrupa ligi", "konferans ligi", "dÃ¼nya kupasÄ±", "uefa", "fifa", "tff", "ziraat tÃ¼rkiye kupasÄ±", "beÅŸiktaÅŸ", "fenerbahÃ§e", "galatasaray", "trabzonspor", "baÅŸakÅŸehir", "milli takÄ±m", "bizim Ã§ocuklar", "voleybol", "filenin sultanlarÄ±", "filenin efeleri", "eczacÄ±baÅŸÄ±", "vakÄ±fbank", "fenerbahÃ§e opet", "basketbol", "12 dev adam", "anadolu efes", "fenerbahÃ§e beko", "nba", "euroleague", "gÃ¼reÅŸ", "yaÄŸlÄ± gÃ¼reÅŸ", "kÄ±rkpÄ±nar", "baÅŸpehlivan", "boks", "kick boks", "tekvando", "karate", "mma", "ufc", "halter", "atÄ±cÄ±lÄ±k", "okÃ§uluk", "mete gazoz", "yusuf dikeÃ§", "formula 1", "tenis", "atletizm", "real madrid", "barcelona", "manchester city", "liverpool", "bayern mÃ¼nih", "psg", "juventus", "inter", "milan", "teknik direktÃ¶r"],
    "medium": ["derbi", "penaltÄ±", "frikik", "korner", "ofsayt", "hat-trick", "var incelemesi", "sarÄ± kart", "kÄ±rmÄ±zÄ± kart", "rÃ¶vanÅŸ", "fikstÃ¼r", "gol kralÄ±", "asist", "smaÃ§", "blok", "manÅŸet", "servis", "ribaund", "Ã¼Ã§lÃ¼k", "nakavt", "raund", "madalya", "altÄ±n madalya", "gÃ¼mÃ¼ÅŸ madalya", "bronz madalya", "ÅŸampiyonluk yarÄ±ÅŸÄ±"],
    "low": ["maÃ§", "karÅŸÄ±laÅŸma", "mÃ¼sabaka", "turnuva", "lig", "sezon", "ÅŸampiyon", "kupa", "galibiyet", "maÄŸlubiyet", "beraberlik", "skor", "puan", "rekor", "performans", "kadro", "transfer", "sÃ¶zleÅŸme", "taraftar", "tribÃ¼n", "takÄ±m", "kulÃ¼p", "antrenÃ¶r", "hakem", "oyuncu", "sakatlandÄ±", "ceza aldÄ±", "finale Ã§Ä±ktÄ±", "kazandÄ±", "kaybetti", "antrenman"]
}
ECONOMY_KEYWORDS = {
    "high": ["vergi", "bÃ¼tÃ§e", "aÃ§Ä±k", "cari aÃ§Ä±k", "enflasyon", "faiz", "zam", "maaÅŸ", "bist 100", "bist 30", "borsa istanbul", "viop", "spk", "kap", "halka arz", "temettÃ¼", "nasdaq", "dow jones", "s&p 500", "fed", "powell", "ecb", "imf", "dÃ¼nya bankasÄ±", "merkez bankasÄ±", "tcmb", "bitcoin", "btc", "ethereum", "eth", "kripto", "blockchain", "binance", "coinbase", "gram altÄ±n", "Ã§eyrek altÄ±n", "ons altÄ±n", "dÃ¶viz", "dolar/tl", "euro/tl", "brent petrol", "akaryakÄ±t", "benzin", "motorin", "tÃ¼praÅŸ", "thy", "aselsan", "ereÄŸli", "kardemir", "sasa", "hektaÅŸ", "banka", "kredi"],
    "medium": ["tÃ¼fe", "Ã¼fe", "politika faizi", "kur korumalÄ±", "sterlin", "dÄ±ÅŸ ticaret", "ihracat", "ithalat", "gsyh", "bÃ¼yÃ¼me rakamlarÄ±", "iÅŸsizlik oranÄ±", "istihdam", "konut satÄ±ÅŸlarÄ±", "kredi notu", "hazine", "kdv", "Ã¶tv", "stopaj", "matrah", "fiyat artÄ±ÅŸÄ±"],
    "low": ["fiyat", "artÄ±ÅŸ", "dÃ¼ÅŸÃ¼ÅŸ", "rekor", "satÄ±ÅŸ", "alÄ±ÅŸ", "yatÄ±rÄ±m", "tasarruf", "borÃ§", "ÅŸirket", "piyasa", "analiz", "beklenti", "hedef", "kar", "zarar", "maliyet", "Ã¼cret", "asgari Ã¼cret", "emekli", "memur"]
}
TECHNOLOGY_KEYWORDS = {
    "high": ["apple", "google", "microsoft", "amazon", "meta", "facebook", "twitter", "x", "instagram", "tiktok", "openai", "chatgpt", "gemini", "nvidia", "intel", "amd", "samsung", "huawei", "xiaomi", "sony", "tesla", "spacex", "nasa", "tÃ¼bitak", "aselsan", "baykar", "tusaÅŸ", "yapay zeka", "ai", "machine learning", "siber gÃ¼venlik", "hacker", "bulut biliÅŸim", "5g", "6g", "uydu", "uzay", "mars", "roket", "astronot", "algoritma", "kodlama"],
    "medium": ["yazÄ±lÄ±m", "donanÄ±m", "iÅŸletim sistemi", "android", "ios", "windows", "linux", "macos", "uygulama", "app", "akÄ±llÄ± telefon", "tablet", "laptop", "bilgisayar", "konsol", "playstation", "xbox", "video oyunu", "espor", "iÅŸlemci", "ram", "ekran kartÄ±", "batarya", "piksel", "gÃ¼ncelleme", "sÃ¼rÃ¼m", "beta", "elektrikli araÃ§", "otonom", "robot", "drone"],
    "low": ["cihaz", "teknoloji", "dijital", "sanal", "platform", "ÅŸifre", "baÄŸlantÄ±", "hÄ±z", "ekran", "butona", "tÄ±kla", "indir", "yÃ¼kle"]
}
POLITICS_KEYWORDS = {
    "high": ["cumhurbaÅŸkanÄ±", "baÅŸÚ©Ø§Ù†", "erdoÄŸan", "Ã¶zgÃ¼r Ã¶zel", "bahÃ§eli", "imamoÄŸlu", "mansur yavaÅŸ", "ak parti", "akp", "chp", "mhp", "iyi parti", "dem parti", "tbmm", "meclis", "parlamento", "bakan", "bakanlÄ±ÄŸÄ±", "kabine", "hÃ¼kÃ¼met", "muhalefet", "iktidar", "seÃ§im", "sandÄ±k", "oy", "ysk", "anayasa", "kararname", "resmi gazete", "nato", "bm", "birleÅŸmiÅŸ milletler", "ab", "avrupa birliÄŸi", "biden", "trump", "putin", "zelenskiy", "diplomasi", "dÄ±ÅŸiÅŸleri", "iÃ§iÅŸleri", "mgk", "milli gÃ¼venlik kurulu", "belediye", "belediye baÅŸkanÄ±", "yerel yÃ¶netim", "kayyum"],
    "medium": ["miting", "aday", "ittifak", "genel baÅŸkan", "grup toplantÄ±sÄ±", "Ã¶nerge", "yasa", "kanun", "teklif", "komisyon", "bÃ¼yÃ¼kelÃ§i", "konsolos", "zirve", "gÃ¶rÃ¼ÅŸme", "temas", "heyet", "sÃ¶zcÃ¼", "parti", "tÃ¼zÃ¼k", "kurultay", "kongre", "referandum"],
    "low": ["aÃ§Ä±klama", "toplantÄ±", "karar", "kriz", "gÃ¼ndem", "lider", "ziyaret", "mesaj", "Ã§aÄŸrÄ±", "tepki", "eleÅŸtiri", "destek", "protesto"]
}
ART_KEYWORDS = {
    "high": ["sinema", "film", "dizi", "tiyatro", "konser", "festival", "sergi", "mÃ¼ze", "sanat", "kÃ¼ltÃ¼r", "edebiyat", "kitap", "yazar", "ÅŸair", "ressam", "heykeltraÅŸ", "oyuncu", "aktris", "aktÃ¶r", "ÅŸarkÄ±cÄ±", "mÃ¼zisyen", "albÃ¼m", "ÅŸarkÄ±", "klip", "single", "netflix", "disney", "blutv", "exxen", "altÄ±n kelebek", "oscar", "emmy", "grammy", "cannes", "altÄ±n portakal", "acun Ä±lÄ±calÄ±", "tarkan", "sezen aksu", "cem yÄ±lmaz", "magazin", "Ã¼nlÃ¼", "dÃ¶vme", "tattoo", "estetik", "fenomen", "sosyal medya"],
    "medium": ["vizyon", "gala", "sahne", "performans", "yÃ¶netmen", "senarist", "yapÄ±mcÄ±", "baÅŸrol", "fragman", "bÃ¶lÃ¼m", "sezon finali", "reyting", "dedikodu", "aÅŸk", "ayrÄ±lÄ±k", "evlilik", "boÅŸanma", "konser takvimi", "bilet", "giÅŸe", "paylaÅŸÄ±m", "takipÃ§i"],
    "low": ["izle", "dinle", "eÄŸlence", "ÅŸov", "yÄ±ldÄ±z", "popÃ¼ler", "trend", "moda", "tarz", "stil", "kÄ±rmÄ±zÄ± halÄ±"]
}

NEGATIVE_KEYWORDS = {
    "political_dominance": {
        "dominant_category": "Siyaset", 
        "keywords": ["resmi gazete", "kararname", "kanun teklifi", "anayasa mahkemesi", "tbmm genel kurulu", "yargÄ±tay", "danÄ±ÅŸtay"],
        "penalty": -40, "affects": ["Spor", "Sanat", "Teknoloji", "GÃ¼ndem"],
        "soft_penalty": -15, "soft_affects": ["Ekonomi"] 
    },
    "sports_dominance": {
        "dominant_category": "Spor", 
        "keywords": ["maÃ§ sonucu", "puan durumu", "fikstÃ¼r", "gol kralÄ±", "sarÄ± kart", "kÄ±rmÄ±zÄ± kart", "teknik direktÃ¶r"],
        "penalty": -40, "affects": ["Siyaset", "Ekonomi", "Teknoloji", "Sanat"],
        "soft_penalty": -10, "soft_affects": ["GÃ¼ndem"] 
    },
    "economic_dominance": {
        "dominant_category": "Ekonomi", 
        "keywords": ["borsa istanbul", "bist 100", "faiz kararÄ±", "enflasyon raporu", "dÃ¶viz kurlarÄ±", "Ã§eyrek altÄ±n"],
        "penalty": -30, "affects": ["Spor", "Sanat", "GÃ¼ndem"],
        "soft_penalty": -10, "soft_affects": ["Siyaset", "Teknoloji"] 
    }
}

# ==========================================
# HELPER FUNCTIONS
# ==========================================
def normalize_turkish(text: str) -> str:
    text = text.replace('Ä°', 'i').replace('I', 'Ä±').replace('Ä', 'ÄŸ').replace('Ãœ', 'Ã¼').replace('Å', 'ÅŸ').replace('Ã–', 'Ã¶').replace('Ã‡', 'Ã§')
    return text.lower()

def calculate_score(text: str, keywords_dict: dict) -> int:
    text = normalize_turkish(text)
    score = 0
    for word in keywords_dict["high"]:
        if word in text:
            score += 50
            break 
    medium_hits = 0
    for word in keywords_dict["medium"]:
        if word in text:
            score += 15
            medium_hits += 1
            if medium_hits >= 2: break
    for word in keywords_dict["low"]:
        if re.search(r'\b' + re.escape(word) + r'\b', text): score += 5
    return score

def apply_negative_penalties(text: str, scores: dict) -> dict:
    text = normalize_turkish(text)
    text = re.sub(r'\s+', ' ', text)
    current_winner = max(scores, key=scores.get) if scores else None
    
    for group, config in NEGATIVE_KEYWORDS.items():
        if config.get("dominant_category") == current_winner: continue
        keywords = config["keywords"]
        hard_penalty = config["penalty"]
        hard_affects = config["affects"]
        soft_penalty = config.get("soft_penalty", -10)
        soft_affects = config.get("soft_affects", [])
        
        found = False
        for word in keywords:
            if re.search(r'\b' + re.escape(word) + r'\b', text):
                found = True
                break 
        
        if found:
            for cat in hard_affects:
                if cat in scores: scores[cat] = max(0, scores[cat] + hard_penalty)
            for cat in soft_affects:
                if cat in scores: scores[cat] = max(0, scores[cat] + soft_penalty)
    return scores

def decide_final_category(ai_category: str, scores: dict) -> str:
    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    top_cat, top_score = sorted_scores[0]
    
    override_threshold = 70 if top_cat == "Ekonomi" else 60

    if scores.get(ai_category, 0) < 15 and top_score > override_threshold:
        print(f"   ğŸ›¡ï¸ GUARD: AI chose '{ai_category}' (Low Score) but keywords strongly suggest '{top_cat}' (Score: {top_score}). Overriding.")
        return top_cat, True 

    return ai_category, False

def generate_unique_slug(db, base_title, trend_id):
    """
    ØªÙˆÙ„ÛŒØ¯ Ø§Ø³Ù„Ø§Ú¯ ÛŒÚ©ØªØ§ Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    """
    base_slug = slugify_turkish(base_title)
    unique_slug = base_slug
    counter = 1
    
    while True:
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ø§Ø³Ù„Ø§Ú¯ ØªÙˆØ³Ø· Ø±Ú©ÙˆØ±Ø¯ Ø¯ÛŒÚ¯Ø±ÛŒ (ØºÛŒØ± Ø§Ø² Ø®ÙˆØ¯Ù Ø§ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯) Ø§Ø´ØºØ§Ù„ Ø´Ø¯Ù‡ ÛŒØ§ Ø®ÛŒØ±
        existing = db.query(Trend).filter(Trend.slug == unique_slug, Trend.id != trend_id).first()
        if not existing:
            return unique_slug
        
        unique_slug = f"{base_slug}-{counter}"
        counter += 1

# ==========================================
# GEMINI LOGIC
# ==========================================
def generate_summary_with_gemini(text_cluster, scores_context):
    if not client or not MODEL_NAME:
        return None, 0, 0, 0 

    prompt = f"""
    You are a professional Turkish news editor. Analyze the following raw text data.
    
    SYSTEM SCORES (For Context Only):
    {scores_context}

    CRITICAL RULES FOR CATEGORIZATION (Reasoning Required):
    1. **Ekonomi**: Currency, Stock Market, Inflation, Taxes, Corporate Finance. (NOT Building collapses/Physical damage).
    2. **Teknoloji**: Software, AI, Hardware, Space, Cyber Security. (NOT Social Media Celebrities/Influencers/Tattoos).
    3. **Sanat/Magazin**: Celebrities, Tattoos, Singers, Movies, Social Media Trends.
    4. **GÃ¼ndem/YaÅŸam**: Accidents, Earthquakes, Weather, Local News, Building Collapses, Animal Attacks.

    INSTRUCTION:
    First, think step-by-step about WHY this news belongs to a category. Write this in 'category_reasoning'.
    Then, select the final 'category'.

    OUTPUT FORMAT: JSON
    {{
        "detected_language": "TR", 
        "is_relevant_to_turkey": true, 
        "headline": "City Name: Short Catchy Headline",
        "summary": "Neutral summary...",
        "category_reasoning": "Explain why you chose the category here...",
        "category": "Chosen Category"
    }}

    TEXT TO ANALYZE:
    {text_cluster}
    """

    max_retries = 3
    base_delay = 1 

    for attempt in range(max_retries):
        try:
            req_start = time.time()
            
            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type='application/json',
                    temperature=0.1,
                    max_output_tokens=500,
                )
            )
            
            req_end = time.time()
            duration = req_end - req_start
            
            in_tok = 0
            out_tok = 0
            if response.usage_metadata:
                in_tok = response.usage_metadata.prompt_token_count
                out_tok = response.usage_metadata.candidates_token_count
                print(f"   ğŸ« Tokens: Input={in_tok}, Output={out_tok} | Time: {duration:.2f}s | Model: {MODEL_NAME}")
            else:
                print(f"   ğŸ Request finished in {duration:.2f}s")
            
            result = json.loads(response.text)
            
            if isinstance(result, list):
                if result and isinstance(result[0], dict):
                    return result[0], in_tok, out_tok, duration
                return None, in_tok, out_tok, duration
                
            return result, in_tok, out_tok, duration

        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                print(f"   â³ Rate Limit Hit (429). Sleeping {base_delay}s... (Attempt {attempt+1}/{max_retries})")
                time.sleep(base_delay)
                base_delay *= 2 
                continue 
            else:
                print(f"   âŒ Gemini Error: {e}")
                return None, 0, 0, 0
    
    print("   âŒ Failed after retries.")
    return None, 0, 0, 0

def process_pending_trends():
    db = SessionLocal()
    try:
        pending_trends = db.query(Trend).filter(
            (Trend.summary == None) | (Trend.summary == ""),
            Trend.message_count >= 1 
        ).order_by(desc(Trend.last_updated)).limit(10).all()

        if not pending_trends: return False

        print(f"âœï¸  Processing {len(pending_trends)} trends with {MODEL_NAME}...")

        for trend in pending_trends:
            time.sleep(0.1) 
            
            news_items = db.query(RawNews).filter(RawNews.trend_id == trend.id).limit(10).all()
            if not news_items: continue

            combined_text = ""
            for n in news_items:
                clean_content = n.content.replace("\n", " ").strip()
                combined_text += f"- {clean_content[:800]}\n"

            scores = {
                "Spor": calculate_score(combined_text, SPORTS_KEYWORDS),
                "Ekonomi": calculate_score(combined_text, ECONOMY_KEYWORDS),
                "Teknoloji": calculate_score(combined_text, TECHNOLOGY_KEYWORDS),
                "Siyaset": calculate_score(combined_text, POLITICS_KEYWORDS),
                "Sanat": calculate_score(combined_text, ART_KEYWORDS)
            }
            scores = apply_negative_penalties(combined_text, scores)
            
            top_score = max(scores.values())
            
            scores_str = "\n".join([f"- {k}: {v}" for k, v in scores.items()])
            system_pre_analysis = f"Ã–N ANALÄ°Z (SÄ°STEM PUANLARI):\n{scores_str}"

            print(f"   ğŸ“Š Trend {trend.id}: Top Score={top_score}")

            ai_result, in_tok, out_tok, duration = generate_summary_with_gemini(combined_text, system_pre_analysis)
            
            status = "Success"
            
            if ai_result:
                is_relevant = ai_result.get("is_relevant_to_turkey", True)
                detected_lang = ai_result.get("detected_language", "EN").upper()
                ai_category = ai_result.get("category", "GÃ¼ndem")
                
                final_category, overridden = decide_final_category(ai_category, scores)
                
                if overridden:
                    status = "Guard Override"

                if final_category == "Teknoloji":
                    text_check = normalize_turkish(combined_text)
                    tech_terms = ["yazÄ±lÄ±m", "yapay zeka", "dijital", "siber", "platform", "uygulama", "algoritma", "kodlama", "internet", "veri", "biliÅŸim", "inovasyon", "apple", "google", "microsoft", "tesla", "togg", "baykar", "aselsan", "robot", "otomasyon", "uzay", "uydu", "kripto", "blockchain", "teknoloji", "cihaz", "telefon", "bilgisayar"]
                    has_strong_tech = any(re.search(r'\b' + re.escape(w) + r'\b', text_check) for w in tech_terms)
                    if not has_strong_tech:
                        if ai_category == "Sanat":
                            final_category = "Sanat"
                        else:
                            final_category = "GÃ¼ndem"
                        status = "Tech Guard"

                if trend.score > 20: is_relevant = True
                if "TR" in detected_lang: is_relevant = True

                if is_relevant:
                    raw_title = ai_result.get("headline", trend.title)
                    if raw_title and len(raw_title) > 250:
                        raw_title = raw_title[:250] + "..."
                    
                    trend.title = raw_title
                    trend.summary = ai_result.get("summary", "")
                    trend.category = final_category 
                    
                    # --- Ø¨Ø®Ø´ Ø³Ø¦Ùˆ: ØªÙˆÙ„ÛŒØ¯ Ø§Ø³Ù„Ø§Ú¯ ÛŒÚ©ØªØ§ ---
                    trend.slug = generate_unique_slug(db, trend.title, trend.id)
                    
                    trend.last_updated = datetime.now(timezone.utc).replace(tzinfo=None)

                    # --- Ù…Ù†Ø·Ù‚ Ù¾Ù†Ø§Ù„ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø®Ø¨Ø§Ø± ÙØ§Ù„ ---
                    title_norm = normalize_turkish(trend.title)
                    if any(word in title_norm for word in JUNK_KEYWORDS):
                        print(f"   âš ï¸ Junk Content Detected ({trend.title}). Limiting score to 10.")
                        trend.score = min(trend.score, 10)

                    print(f"   âœ… Summarized: [{trend.category}] {trend.title}")
                    
                    try:
                        # LOGGING
                        log_to_csv(trend.id, MODEL_NAME, in_tok, out_tok, duration, final_category, status)
                        db.commit()

                        # --- Ø³Ø¦Ùˆ: Ø§Ø·Ù„Ø§Ø¹â€ŒØ±Ø³Ø§Ù†ÛŒ Ø¢Ù†ÛŒ Ø¨Ù‡ Ú¯ÙˆÚ¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¯ Ù†ØµØ§Ø¨ Ø§Ù…ØªÛŒØ§Ø² ---
                        if trend.slug:
                            if trend.score >= GOOGLE_INDEXING_THRESHOLD:
                                target_url = f"{BASE_SITE_URL}/trend/{trend.slug}"
                                success, err_msg = notify_google(target_url)
                                if success:
                                    print(f"   ğŸš€ Google Indexing API: Success for {trend.slug}")
                                else:
                                    print(f"   âš ï¸ Google Indexing API Error for {trend.slug}: {err_msg}")
                            else:
                                print(f"   ğŸ’¤ Indexing Skipped (Score {trend.score} < {GOOGLE_INDEXING_THRESHOLD})")
                                
                    except Exception as e:
                        db.rollback()
                        print(f"   âŒ DB Commit Error for Trend {trend.id}: {e}")
                        continue
                    
                else:
                    trend.is_active = False
                    trend.summary = "Filtered."
                    print(f"   ğŸ—‘ï¸  Filtered Out.")
                    log_to_csv(trend.id, MODEL_NAME, in_tok, out_tok, duration, "Filtered", "Irrelevant")
                    try:
                        db.commit()
                    except:
                        db.rollback()
                
            else:
                # --- FALLBACK ---
                print(f"   âš ï¸ AI Failed for Trend {trend.id}. Setting fallback.")
                fallback_category = max(scores, key=scores.get) if top_score >= 20 else "GÃ¼ndem"
                first_news = news_items[0].content.strip()
                fallback_summary = ' '.join(first_news.split()[:40]) + "..."
                
                trend.summary = fallback_summary
                trend.title = trend.title or fallback_summary[:100]
                trend.category = fallback_category
                trend.last_updated = datetime.now(timezone.utc).replace(tzinfo=None)
                
                # ØªÙˆÙ„ÛŒØ¯ Ø§Ø³Ù„Ø§Ú¯ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª ÙØ§Ù„â€ŒØ¨Ú©
                trend.slug = generate_unique_slug(db, trend.title, trend.id)
                
                print(f"   ğŸ”° Fallback Applied: [{fallback_category}]")
                log_to_csv(trend.id, MODEL_NAME, 0, 0, 0, fallback_category, "Fallback (AI Fail)")
                try:
                    db.commit()
                except:
                    db.rollback()

        return True
    finally:
        db.close()

def main():
    print("ğŸ¤– Cloud AI Worker Starting (Turbo Mode + CSV Logging + SEO Indexing)...")
    while True:
        try:
            did_work = process_pending_trends()
            sleep_time = 1 if did_work else 10
            if not did_work: print("ğŸ’¤ Waiting...", end='\r')
            time.sleep(sleep_time)
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"âŒ Global Worker Error: {e}")
            time.sleep(10)

if __name__ == "__main__":
    main()
